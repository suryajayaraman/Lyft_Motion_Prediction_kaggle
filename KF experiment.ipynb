{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBARRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
    "from tqdm import tqdm\n",
    "\n",
    "import l5kit\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l5kit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET CONFIG PARAMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lyft configs ---\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'data_path': \"lyft-motion-prediction-autonomous-vehicles/\",\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet34',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'model_name': \"model_resnet34_output\",\n",
    "        'lr': 1e-3,\n",
    "        'weight_path': \"submissions/Baseline_Resnet34/model_multi_update_lyft_public_resnet34.pth\",\n",
    "        'train': False,\n",
    "        'predict': True\n",
    "    },\n",
    "\n",
    "    'raster_params': {\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "    \n",
    "    'sample_data_loader': {\n",
    "        'key': 'scenes/sample.zarr',\n",
    "        'batch_size': 1,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 16,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 1,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'train_params': {\n",
    "        'max_num_steps': 101,\n",
    "        'checkpoint_every_n_steps': 20,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "DIR_INPUT = cfg[\"data_path\"]\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET AND DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "sample_cfg = cfg['sample_data_loader']\n",
    "sample_zarr = ChunkedDataset(dm.require(sample_cfg[\"key\"])).open()\n",
    "sample_AgentDataset = AgentDataset(cfg, sample_zarr, rasterizer)\n",
    "sample_dataloader = DataLoader(sample_AgentDataset,shuffle=sample_cfg[\"shuffle\"],batch_size=sample_cfg[\"batch_size\"],\n",
    "                             num_workers=sample_cfg[\"num_workers\"])\n",
    "\n",
    "print(sample_AgentDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length is  111634\n",
      "Dataloader length is  111634\n"
     ]
    }
   ],
   "source": [
    "print('Dataset length is ', len(sample_AgentDataset))\n",
    "print('Dataloader length is ', len(sample_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function utils ---\n",
    "# Original code from https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/metrics.py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute a negative log-likelihood for the multi-modal scenario.\n",
    "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    https://leimao.github.io/blog/LogSumExp/\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
    "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_single(\n",
    "    gt: Tensor, pred: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
    "    # create confidence (bs)x(mode=1)\n",
    "    batch_size, future_len, num_coords = pred.shape\n",
    "    confidences = pred.new_ones((batch_size, 1))\n",
    "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftMultiModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: Dict, num_modes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        architecture = cfg[\"model_params\"][\"model_architecture\"]\n",
    "        #backbone = eval(architecture)(pretrained=True, progress=True)\n",
    "        backbone = eval(architecture)(pretrained=True)\n",
    "        self.backbone = backbone\n",
    "\n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # This is 512 for resnet18 and resnet34;\n",
    "        # And it is 2048 for the other resnets\n",
    "        \n",
    "        if architecture == \"resnet50\":\n",
    "            backbone_out_features = 2048\n",
    "        else:\n",
    "            backbone_out_features = 512\n",
    "\n",
    "        # X, Y coords for the future positions (output shape: batch_sizex50x2)\n",
    "        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        num_targets = 2 * self.future_len\n",
    "\n",
    "        # You can add more layers here.\n",
    "        self.head = nn.Sequential(\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "        )\n",
    "\n",
    "        self.num_preds = num_targets * num_modes\n",
    "        self.num_modes = num_modes\n",
    "\n",
    "        self.logit = nn.Linear(4096, out_features=self.num_preds + num_modes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.head(x)\n",
    "        x = self.logit(x)\n",
    "\n",
    "        # pred (batch_size)x(modes)x(time)x(2D coords)\n",
    "        # confidences (batch_size)x(modes)\n",
    "        bs, _ = x.shape\n",
    "        pred, confidences = torch.split(x, self.num_preds, dim=1)\n",
    "        pred = pred.view(bs, self.num_modes, self.future_len, 2)\n",
    "        assert confidences.shape == (bs, self.num_modes)\n",
    "        confidences = torch.softmax(confidences, dim=1)\n",
    "        return pred, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion = pytorch_neg_multi_log_likelihood_batch):\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    # Forward pass\n",
    "    preds, confidences = model(inputs)\n",
    "    loss = criterion(targets, preds, confidences, target_availabilities)\n",
    "    return loss, preds, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_points(points2D, ax=None, label=None):\n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    #print(points2D.shape)\n",
    "    ax.plot(points2D[:,0], points2D[:,1], label = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_mode(multi_mode_prediction):\n",
    "    num_modes = multi_mode_prediction.shape[0]\n",
    "    for mode in range(num_modes):\n",
    "        plot_2d_points(multi_mode_prediction[mode], label=str(mode) + 'th mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING MODEL WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded\n",
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ==== INIT MODEL=================\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LyftMultiModel(cfg)\n",
    "\n",
    "#load weight if there is a pretrained model\n",
    "weight_path = cfg[\"model_params\"][\"weight_path\"]\n",
    "if weight_path:\n",
    "    model.load_state_dict(torch.load(weight_path))\n",
    "    print('Model weights loaded')\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg[\"model_params\"][\"lr\"])\n",
    "print(f'device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_it = iter(sample_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(sample_it)\n",
    "_, preds, confidences = forward(data, model, device)\n",
    "#print(preds.shape, confidences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "preds = preds.cpu().detach().numpy()\n",
    "world_from_agents = data[\"world_from_agent\"].numpy()\n",
    "centroids = data[\"centroid\"].numpy()\n",
    "confidences = confidences.cpu().detach().numpy()\n",
    "\n",
    "#print(preds.shape, confidences.shape, centroids.shape)\n",
    "#print(confidences)\n",
    "preds = preds[0]\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2385b57f60>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fig, ax = plt.subplots(figsize = (12,12))\n",
    "#plt.set_aspect('equal', 'box')\n",
    "#ax.set_xlim([-5, 20])\n",
    "#ax.set_ylim([-5, 5])\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plot_multi_mode(preds)\n",
    "plot_2d_points(data['target_positions'].numpy()[0], label = 'target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'target_positions', 'target_yaws', 'target_availabilities', 'history_positions', 'history_yaws', 'history_availabilities', 'world_to_image', 'raster_from_world', 'raster_from_agent', 'agent_from_world', 'world_from_agent', 'track_id', 'timestamp', 'centroid', 'yaw', 'extent'])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(lyft_kaggle)",
   "language": "python",
   "name": "lyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
