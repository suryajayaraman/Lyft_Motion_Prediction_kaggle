{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder Decoder method for Motion prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of the competition is to predict the future trajectories of other vehicles using the past information (Bird's eye view of the scene containing agents detected by perception system, past trajectories lane information, traffic lights etc).\n",
    "\n",
    "\n",
    "Since this is sequential problem, I thought of using LSTM based models. Basic idea is as follows:\n",
    "\n",
    "[LSTM_baseline idea](https://www.kaggle.com/suryajrrafl/lstm-baseline-weights?select=lstm+baseline+idea.jpg)\n",
    "\n",
    "This is my first attempt at kaggle competition, pytorch and LSTM models. Suggestions are most welcome.\n",
    "\n",
    "\n",
    "**REFERENCES**\n",
    "\n",
    "Some helper functions in this notebook were taken from the great public kernels avaiable. \n",
    "\n",
    "[Great reference notebook using Resnet model](https://www.kaggle.com/huanvo/lyft-complete-train-and-prediction-pipeline)\n",
    "\n",
    "[Pytorch baseline train](https://www.kaggle.com/pestipeti/pytorch-baseline-train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
    "import torch.functional as F\n",
    "\n",
    "# l5kit imports\n",
    "import l5kit\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "\n",
    "# common imports\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l5kit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random seed generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of trainable parameters in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_no_of_trainable_params(model):\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    #print(total_trainable_params)\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function utils ---\n",
    "# Original code from https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/metrics.py\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute a negative log-likelihood for the multi-modal scenario.\n",
    "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    https://leimao.github.io/blog/LogSumExp/\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
    "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_single(\n",
    "    gt: Tensor, pred: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
    "    # create confidence (bs)x(mode=1)\n",
    "    batch_size, future_len, num_coords = pred.shape\n",
    "    confidences = pred.new_ones((batch_size, 1))\n",
    "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs - constants for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_LENGTH        = 50\n",
    "NUMBER_OF_HISTORY_FRAMES = 10\n",
    "RASTER_IMG_SIZE          = 224\n",
    "PIXEL_SIZE               = 0.5\n",
    "NUM_MODES                = 1\n",
    "NUMBER_OF_FUTURE_FRAMES = 50\n",
    "\n",
    "### TRAIN FROM WHERE LEFT OFF, CHANGE THE STARTING INDICES VARIABLE ACCORDINGLY\n",
    "TRAIN_START_INDICES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lyft configs ---\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'data_path': \"lyft-motion-prediction-autonomous-vehicles/\",\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet18',\n",
    "        'history_num_frames': NUMBER_OF_HISTORY_FRAMES,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': PREDICTION_LENGTH,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'model_name': \"LSTM_EncDec_R18\",\n",
    "        'weight_path': \"\",\n",
    "        'lr': 1e-3,\n",
    "        'train': True,\n",
    "        'predict': False\n",
    "    },\n",
    "\n",
    "    'raster_params': {\n",
    "        'raster_size': [RASTER_IMG_SIZE, RASTER_IMG_SIZE],\n",
    "        'pixel_size': [PIXEL_SIZE, PIXEL_SIZE],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "\n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 16,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 32,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'sample_data_loader': {\n",
    "        'key': 'scenes/sample.zarr',\n",
    "        'batch_size': 16,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'train_params': {\n",
    "        'train_start_index' : 0,\n",
    "        'max_num_steps': 25,\n",
    "        'checkpoint_every_n_steps': 5,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder Decoder Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_EncDec(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: Dict, output_size, embed_size, use_other_features= False):\n",
    "        super(LSTM_EncDec, self).__init__()\n",
    "\n",
    "        # Resnet architecture as such for most part of encoding\n",
    "        architecture = cfg[\"model_params\"][\"model_architecture\"]\n",
    "        backbone = eval(architecture)(pretrained=True)\n",
    "        self.backbone = backbone\n",
    "\n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # This is 512 for resnet18 and resnet34; And it is 2048 for the other resnets\n",
    "        if architecture == \"resnet50\":\n",
    "            backbone_out_features = 2048\n",
    "        else:\n",
    "            backbone_out_features = 512\n",
    "            \n",
    "        # X, Y coords for the future positions (output shape: batch_sizex50x2)\n",
    "        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        #num_targets = 2 * self.future_len\n",
    "        #self.num_targets = num_targets\n",
    "        \n",
    "        # including other feautres as part of encoded hidden state\n",
    "        other_agent_features = num_history_channels + 6\n",
    "        \n",
    "        # LSTM layers\n",
    "        #self.output_size = output_size\n",
    "        #self.embed_size = embed_size\n",
    "        \n",
    "        if use_other_features ==True:\n",
    "            self.hidden_size = backbone_out_features + other_agent_features\n",
    "        else:\n",
    "            self.hidden_size = backbone_out_features\n",
    "        \n",
    "        self.linear_in  = nn.Linear(output_size, embed_size)\n",
    "        self.decoder    = nn.LSTMCell(embed_size, self.hidden_size, bias=True)\n",
    "        self.linear_out = nn.Linear(self.hidden_size, output_size) \n",
    "            \n",
    "    def Resnet_forward(self, x):\n",
    "        # Resnet encoding layers\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # input size to embedded size\n",
    "        embedded = F.relu(self.linear_in(x))\n",
    "        \n",
    "        # passsing through LSTM layer\n",
    "        LSTM_out = self.decoder(embedded, hidden)\n",
    "        \n",
    "        # converting from hidden state size to output size\n",
    "        prediction = self.linear_out(LSTM_out[0])\n",
    "        \n",
    "        return prediction, LSTM_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion = pytorch_neg_multi_log_likelihood_batch):\n",
    "    # extracting required array elements and moving to device\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
    "    targets = data[\"target_positions\"].to(device)    \n",
    "    history_positions = data['history_positions'].to(device).view(TRAIN_BATCH_SIZE, -1)\n",
    "    centroid = data['centroid'].to(device).float()\n",
    "    yaw = data['yaw'].to(device).view(TRAIN_BATCH_SIZE, 1).float()\n",
    "    extent = data['extent'].to(device)\n",
    "    \n",
    "    batch_size = inputs.shape[0]    \n",
    "    \n",
    "    # concatenating vehicle data\n",
    "    agent_data = torch.cat((history_positions, centroid, yaw, extent), dim=1)\n",
    "    \n",
    "    # concatenating image data and vehicle data to get encoded state\n",
    "    image_data = model.Resnet_forward(inputs)\n",
    "    decoder_hidden = torch.cat((image_data, agent_data), dim=1)\n",
    "    \n",
    "    # preallocating memory\n",
    "    prediction = torch.zeros_like(targets).to(device)\n",
    "    loss = 0.0\n",
    "    \n",
    "    # change to last history position?\n",
    "    decoder_input = torch.zeros(batch_size, 1, 2).to(device) \n",
    "\n",
    "    # Decode hidden state in future trajectory\n",
    "    for t in range(PREDICTION_LENGTH):\n",
    "        decoder_output, decoder_hidden = model(decoder_input, decoder_hidden)\n",
    "        prediction[:, t, :] = decoder_output\n",
    "\n",
    "        # Update loss\n",
    "        loss += criterion(decoder_output[:, :2], targets[:, di, :], target_availabilities[:])\n",
    "\n",
    "        # Use own predictions as inputs at next step\n",
    "        decoder_input = decoder_output\n",
    "    \n",
    "    # Get average loss for pred_len\n",
    "    loss = loss / PREDICTION_LENGTH \n",
    "        \n",
    "    return loss, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "DIR_INPUT = cfg[\"data_path\"]\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager(None)\n",
    "rasterizer = build_rasterizer(cfg, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT TRAIN DATASET============================================================\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train dataset is  22496709\n",
      "==================================TRAIN DATA==================================\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "print('Length of Train dataset is ' ,len(train_dataset))\n",
    "print(\"==================================TRAIN DATA==================================\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22496709"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before slicing, start indices are  [ 4544818 11783688  2821987 10532781  1993967 15711361  7726097   547516\n",
      " 17556178  7275034]\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = np.random.choice(len(train_dataset), size = len(train_dataset), replace = False)\n",
    "print('Before slicing, start indices are ', sampled_indices[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_START_INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After slicing, start indices are  [ 4544818 11783688  2821987 10532781  1993967 15711361  7726097   547516\n",
      " 17556178  7275034]\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = sampled_indices[TRAIN_START_INDICES:]\n",
    "print('After slicing, start indices are ', sampled_indices[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasampler = SubsetRandomSampler(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=Datasampler, batch_size=train_cfg[\"batch_size\"], \n",
    "                             num_workers=train_cfg[\"num_workers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA device && encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ==== INIT MODEL=================\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on hyperparameters\n",
    "embed_size = 8\n",
    "#n_layers   = 2\n",
    "\n",
    "# instantiate an RNN\n",
    "model = LSTM_EncDec(cfg, embed_size, output_size, use_other_features=True)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78400"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_no_of_trainable_params(model.backbone.conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11689512"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = resnet18(pretrained=True)\n",
    "find_no_of_trainable_params(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  11758504 parameters in Backbone of LSTM model\n",
      "There are  1530155 parameters in rest of LSTM model\n"
     ]
    }
   ],
   "source": [
    "Total_model_params = find_no_of_trainable_params(model)\n",
    "Backbone_params = find_no_of_trainable_params(model.backbone)\n",
    "print(f'There are  {Backbone_params} parameters in Backbone of LSTM model')\n",
    "print(f'There are  {Total_model_params - Backbone_params} parameters in rest of LSTM model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the pretrained weights\n",
    "#model.load_state_dict(torch.load(cfg['model_params']['weight_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adam optimiser function\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg[\"model_params\"][\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_START_INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ABOUT TO START ... FROM  0   BATCH\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING ABOUT TO START ... FROM ', TRAIN_START_INDICES, '  BATCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TRAINING LOOP =========================================================\n",
    "if cfg[\"model_params\"][\"train\"]:\n",
    "    \n",
    "    tr_it = iter(train_dataloader)\n",
    "    progress_bar = tqdm(range(TRAIN_START_INDICES, \n",
    "                              TRAIN_START_INDICES + cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "    num_iter = cfg[\"train_params\"][\"max_num_steps\"]\n",
    "    losses_train = []\n",
    "    iterations = []\n",
    "    metrics = []\n",
    "    times = []\n",
    "    model_name = cfg[\"model_params\"][\"model_name\"]\n",
    "    start = time.time()\n",
    "    hidden_state = None\n",
    "    \n",
    "    for i in progress_bar:\n",
    "        try:\n",
    "            data = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(train_dataloader)\n",
    "            data = next(tr_it)\n",
    "            \n",
    "        # Forward pass\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        loss, hidden_state, _, _ = forward(data, model, hidden_state, device)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_train.append(loss.item())\n",
    "\n",
    "        progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
    "        if i % cfg['train_params']['checkpoint_every_n_steps'] == 0:\n",
    "            sample_number = i * cfg['train_data_loader']['batch_size']            \n",
    "            state = {\n",
    "              'state_dict': model.state_dict(),\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, f'{model_name}_{sample_number}k.pth')\n",
    "            iterations.append(i)\n",
    "            metrics.append(np.mean(losses_train))\n",
    "            times.append((time.time()-start)/60)\n",
    "\n",
    "    sample_number = i * cfg['train_data_loader']['batch_size']\n",
    "    results = pd.DataFrame({'iterations': iterations, 'metrics (avg)': metrics, 'elapsed_time (mins)': times})\n",
    "    results.to_csv(f\"train_metrics_{model_name}_{sample_number}k.csv\", index = False)\n",
    "    train_losses_csv = pd.DataFrame({'iteration': TRAIN_START_INDICES + np.arange(len(losses_train)), \n",
    "                                 'losses_train': losses_train})\n",
    "    train_losses_csv.to_csv(f\"train_losses_{model_name}_{sample_number}k.csv\", index = False)\n",
    "    print(f\"Total training time is {(time.time()-start)/60} mins\")\n",
    "    print(results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
