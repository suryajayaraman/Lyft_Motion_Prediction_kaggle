{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Installing l5kit library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Utility script is used for l5kit training and inference purposes. Else GPUs cannot be used due to bug in kaggle notebook. \n\n**Thanks Peter** [Unofficial fix for l5kit](https://www.kaggle.com/pestipeti/lyft-l5kit-unofficial-fix) for the utility script"},{"metadata":{},"cell_type":"markdown","source":"## Library import"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.resnet import resnet18\nfrom tqdm import tqdm\nfrom typing import Dict\n\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.rasterization import build_rasterizer","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR_INPUT = \"../input/lyft-motion-prediction-autonomous-vehicles/\"\n\nSINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}/single_mode_sample_submission.csv\"\nMULTI_MODE_SUBMISSION = f\"{DIR_INPUT}/multi_mode_sample_submission.csv\"\n\nDEBUG = False","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training configuration parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg = {\n    'format_version': 4,\n    'model_params': {\n        'model_architecture': 'resnet18',\n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [300, 300],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map/aerial_map.png',\n        'semantic_map_key': 'semantic_map/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    \n    'train_data_loader': {\n        'key': 'scenes/train.zarr',\n        'batch_size': 32,\n        'shuffle': True,\n        'num_workers': 4\n    },\n    \n    'train_params': {\n        'max_num_steps': 100 if DEBUG else 2000,\n        'checkpoint_every_n_steps': 500,\n        \n        # 'eval_every_n_steps': -1\n    }\n}","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"{'format_version': 4,\n 'model_params': {'model_architecture': 'resnet18',\n  'history_num_frames': 10,\n  'history_step_size': 1,\n  'history_delta_time': 0.1,\n  'future_num_frames': 50,\n  'future_step_size': 1,\n  'future_delta_time': 0.1},\n 'raster_params': {'raster_size': [300, 300],\n  'pixel_size': [0.5, 0.5],\n  'ego_center': [0.25, 0.5],\n  'map_type': 'py_semantic',\n  'satellite_map_key': 'aerial_map/aerial_map.png',\n  'semantic_map_key': 'semantic_map/semantic_map.pb',\n  'dataset_meta_key': 'meta.json',\n  'filter_agents_threshold': 0.5},\n 'train_data_loader': {'key': 'scenes/train.zarr',\n  'batch_size': 32,\n  'shuffle': True,\n  'num_workers': 4},\n 'train_params': {'max_num_steps': 2000, 'checkpoint_every_n_steps': 500}}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\ndm = LocalDataManager(None)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset and Dataset iterator"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cfg = cfg[\"train_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Train dataset/dataloader\ntrain_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\ntrain_dataset = AgentDataset(cfg, train_zarr, rasterizer)\nprint(train_dataset)","execution_count":6,"outputs":[{"output_type":"stream","text":"+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n| Num Scenes | Num Frames | Num Agents | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n|   16265    |  4039527   | 320124624  |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset,\n                              shuffle=train_cfg[\"shuffle\"],\n                              batch_size=train_cfg[\"batch_size\"],\n                              num_workers=train_cfg[\"num_workers\"])","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(train_zarr))    #l5kit.data.zarr_dataset.ChunkedDataset\nprint(type(train_dataset)) #l5kit.dataset.agent.AgentDataset","execution_count":8,"outputs":[{"output_type":"stream","text":"<class 'l5kit.data.zarr_dataset.ChunkedDataset'>\n<class 'l5kit.dataset.agent.AgentDataset'>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"A dataset instance contains information of ego vehicle and agents across frames (in key 'image'), \nvehicle level information in 'target_positions', 'target_yaws', 'history_positions', 'history_yaws'"},{"metadata":{},"cell_type":"markdown","source":"## IMAGE of Train dataset\n\n* image_size = (num_in_channels, raster_size_height, raster_size_width)\n* num_in_channels = num_history_channels + 3\n* num_history_channels = (num_history_frames + 1) * 2 \n* (past frames + current frame) * (one for ego vehicle + one for agents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_dataset[0]['image'].shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"(25, 300, 300)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Train model class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LyftModel(nn.Module):\n    \n    def __init__(self, cfg: Dict):\n        super().__init__()\n        \n        self.backbone = resnet18(pretrained=True, progress=True)\n        \n        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n        num_in_channels = 3 + num_history_channels\n\n        self.backbone.conv1 = nn.Conv2d(\n            num_in_channels,\n            self.backbone.conv1.out_channels,\n            kernel_size=self.backbone.conv1.kernel_size,\n            stride=self.backbone.conv1.stride,\n            padding=self.backbone.conv1.padding,\n            bias=False,\n        )\n        \n        # This is 512 for resnet18 and resnet34;\n        # And it is 2048 for the other resnets\n        backbone_out_features = 512\n\n        # X, Y coords for the future positions (output shape: Bx50x2)\n        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n\n        # You can add more layers here.\n        self.head = nn.Sequential(\n            # nn.Dropout(0.2),\n            nn.Linear(in_features=backbone_out_features, out_features=4096),\n        )\n\n        self.logit = nn.Linear(4096, out_features=num_targets)\n        \n    def forward(self, x):\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x)\n\n        x = self.backbone.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.head(x)\n        x = self.logit(x)\n        \n        return x","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ==== INIT MODEL\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = LyftModel(cfg)\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Later we have to filter the invalid steps.\ncriterion = nn.MSELoss(reduction=\"none\")","execution_count":12,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222667091e5a4f93b68fa528255a19d6"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ==== TRAIN LOOP\ntr_it = iter(train_dataloader)\n\nprogress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\nlosses_train = []\n\nfor itr in progress_bar:\n\n    try:\n        data = next(tr_it)\n    except StopIteration:\n        tr_it = iter(train_dataloader)\n        data = next(tr_it)\n\n    model.train()\n    torch.set_grad_enabled(True)\n    \n    # Forward pass\n    inputs = data[\"image\"].to(device)\n    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n    targets = data[\"target_positions\"].to(device)\n    \n    outputs = model(inputs).reshape(targets.shape)\n    loss = criterion(outputs, targets)\n\n    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n    loss = loss * target_availabilities\n    loss = loss.mean()\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses_train.append(loss.item())\n\n    if (itr+1) % cfg['train_params']['checkpoint_every_n_steps'] == 0 and not DEBUG:\n        torch.save(model.state_dict(), f'model_state_{itr}.pth')\n    \n    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train[-100:])}\")","execution_count":14,"outputs":[{"output_type":"stream","text":"loss: 18.389493942260742 loss(avg): 15.574040813446045: 100%|██████████| 2000/2000 [1:36:59<00:00,  2.91s/it]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), f'model_state_last.pth')","execution_count":15,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}